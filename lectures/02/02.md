
##Single core optimizations

<img src="../misc_images/deadpool.png" width="200px">

---

##Recap of what we talked in the last lectures ...

---

# Measure performance

---

`O` notation

>In mathematics, big O notation describes the limiting behavior of a function when the argument tends towards a particular value or infinity, usually in terms of simpler functions.

Describes the eventual shape of the runtime

---

The canonical forms is not that great
* `1000*(n^2)` is most-likely worse than `n^2`
* **operations count != runtime**

Does not takes into account the environment.

---

How do will measure ?

* We will care about the time a particular hardware spends to compute a given task.
 * C-function `clock()` will be good enough for our needs
 * There are other OS specifc and hardware specific options (TSC, queryPerfCounter)

**Getting proper measurements is hard.**

We will have a lecture on how to use tools for profiling later.

---

* Measure enough times
 * Results should be reproducible
* Check the deviation
* Proper hardware - make sure it is not in "power save" mode, does not throttle, is not doing something else at the same time, etc.
* Be aware of the OS hacks - RAM/files might only seem released. In fact they are often released only when this is necessary.
* Restarting the machine might change the results (why?).

---

## 	(Almost) everytime something needs to be optimized, you should start with profiling.

* 90/10 rule
 * If a piece of code takes 2% of the total program execution and you make it 100times faster, you have gained < 2%.
* If there is no "slow" function, you might have wrong design.

---

## The result of the program should always be correct.

Otherwise we can optimize every program to work in O(1)

---

#Single core optimizations

---

## Premature optimization is the root of all evil

### But don't pessimize prematurely, too

If you wonder what your approach should be - use the one that seems to have the most common sense (especially for micro-optimizations). There is a big chance, that the compiler is already taking care (and you risk to confuse it, if you try to be smart).


---

Types of Data Hazards

Data-dependence (**Read-after-Write**):
0. a + b -> c
1. c + d -> e

Anti-dependence (**Write-after-Read**):
0. a + b -> c
1. d + e -> a

Output-dependence (**Write-after-Write**):
0. a + b -> c
1. d + e -> c

What about (Read-after-Read) ?

---

#[DEMO ILP]

#[DEMO CACHES]

---

Translation Look-aside Buffer (**TLB**) maintains a small list of frequently used memory pages and their locations; addressing data that are location on one of these pages is much faster than data that are not. Consequently, one wants to code in such a way that the number of pages accessed is kept low.

What is the **page size** in the modern OSs ? How it is generated ?

*if we have time today, we will check how custom allocators can affect the TLB cache performance*

---

Branch Predictor
```
if (a) {
    //...
} else {
    //...
}
```

```
eval(A)

if (!A) {
    jmp p1
}

//code A
jmp p2
p1:
... code !A
p2:
```

---

```
#if !defined(_MSC_VER) || defined(__INTEL_COMPILER)
    #define   likely(expr) __builtin_expect((expr),true )
    #define unlikely(expr) __builtin_expect((expr),false)
#endif //!defined(_MSC_VER) || defined(__INTEL_COMPILER)
```

```
while (likely(flag1 == 0)) { ... }
//
if (unlikely(x == 0)) { ... }
```

---

Pointer aliasing

![pointer aliasing](../01/images/pointer_aliasing.png)

---

```
void add(int* restrict ptrA, 
         int* restrict ptrB, 
         int* restrict val) 
{
    *ptrA += *val;
    *ptrB += *val;
}
```

C++ ? Fortran ?

Can function inlining affect that ? 

---

#[DEMO POINTER ALIAS]

---

IF vs SWITCH
```
switch(i) {
    case 0: doZero(); break
    case 1: doOne();
    case 2: doTwo(); break
    default: doOther();
}
```

```
if greater, jmp to DEFAULT
compare REG to 0
if less jmp to DEFAULT
jmp to table[REG]
data table
ZERO
ONE
TWO
end data
ZERO: call doZero
jmp END
ONE: call doOne
TWO: call doTwo
jmp END
DEFAULT: call doDefault
END:
```

---

Sometimes compiler makes jump table from IFs too.

As usual, if this is really important, don't trust the compiler and code the jump table by yourself.

---

What about i++ vs ++i ?

```
self_type operator++() { 
    self_type i = *this; 
    ptr_++;
    return i;
}

self_type operator++(int junk) { 
    ptr_++; 
    return *this; 
}
```

Usually doesn't matter, compiler are often smart.

But go for **++i**.

---

What about **\*** vs **/**

```
volatile float f = 196.f;
float y = f / 2.f;
float z = f * .5f;
```
Well, it depends on the compiler options (so check the compiler manual).
```
movss  -0x14(%rbp), %xmm2
divss  %xmm1, %xmm2       ; <---
movss  %xmm2, -0x18(%rbp)
movss  -0x14(%rbp), %xmm1
mulss  %xmm0, %xmm1
```

```
movss  0x50(%rip), %xmm1
mulss  %xmm1, %xmm0      ; <---
movss  -0x4(%rbp), %xmm2
mulss  %xmm1, %xmm2
```

---

```
volatile float a = 196.f;
volatile float f = pow(a, 2.0); //how about pow(a, 2.1) ?
```

```
movss  -0x4(%rbp), %xmm0
mulss  %xmm0, %xmm0
movss  %xmm0, -0x8(%rbp)
```

```
movsd  0x64(%rip), %xmm1
movq   %rax, -0x20(%rbp)
callq  0x100000f58               ; symbol stub for: pow
```

Most of the time compiler does a great job.

---

Inlining is the root of all optimizations

```
void changeSaturation(T *R, T *G, T *B, T change) {
    T P=sqrt(
        (*R)*(*R)*Pr+
        (*G)*(*G)*Pg+
        (*B)*(*B)*Pb ) ;

    *R=P+((*R)-P)*change;
    *G=P+((*G)-P)*change;
    *B=P+((*B)-P)*change; 
}
```

```
float r, g, b, c;
r = g = b = c = 0.f;
changeSaturation(&r, &g, &b, &c);
printf("%f\n", r + g + b + c);
```

---

#[DEMO INLINING]

---

So why just don't we inline everything ?

---

All of this - caches, branch predictor, out of order execution, register renaming, etc is done to hide the latency to the memory.

Are there any other completely different approaches ?

---

#COMPILERS

---

Compilers 

* cl (MSVS)

* icc (Intel)

* gcc 

* clang 

* others

---

* Optimizations vs No-Optimizations ( **-O** )
 * By default, on most compilers they are **off**.
* `./gcc -O3 -mSSE2 -m64`
* Usually `-O0` turns optimizations off
 * Check the compiler manual !

---

![](./images/visual_studio.png)

---

![](./images/xcode.png)

---

What about linking between libraries build from different compilers ?
 * **clang & icc** are living secret double live for quite some time 
___
What about linking between libraries build form different version of the same compiler ?
 * Windows (vc8, vc9, vc91, vc101, vc11)
 * Linux (libstdc++)
 * OS X (libstdc++)

---

Compiler optimized code:
* Runs much faster
* Takes more time to compile
* Can't be debugged with IDE
 * It will generally work, but you will **debug garbage**

---

Rule of thumb:
* **always** do benchmarks with optimizations **on** 
* **always** debug with optimizations **off**.

---

<img style="float: left;" src="../01/images/compiler1.png" alt="compiler dragoon book" width="380px">

<img style="float: right;" src="../01/images/compiler2.png" width="380px">

---

<img style="float: right;" src="../01/images/compiler3.png" width="380px">

* Link time code generation 

* Whole program optimization 

* Link Time Optimization


---


All in one cpp

```
//file unity_build.cpp
#include "first.cpp"
#include "second.cpp"
//...
#include "last.cpp"
```

Pros : 0 link time, fast compilation, possibly faster code (why?)

Cons : hard to maintain

---

![](./images/ir.png)

---

Memory alignment
```
for (int i = 0; i < 4096; ++i) {
    printf("%p\n", new int);
}
```

```
template <typename T>
struct Node {
    T* left; T* right;
    bool something, somethingElse;
};

template<typename T>
struct Node {
T* left, right;
Т* getLeft() const { //make getRight() too
    T* tempPtr = left;
    setFlag(tempPtr, 0, 0);
    setFlag(tempPtr, 1, 0);
    return tempPtr;
}
bool getSomething() const { return getFlag(left, 0); }
bool getSomethingElse() const { return getFlag(left, 1); }
void setSomething(bool foo) { setFlag(left, 0, foo); }
void setSomethingElse(bool foo) { setFlag(left, 1, foo); }
};
```

---

We have to reset the last 2 bits every time when we use the ptr.

Some architectures are **ignoring** them.

---

To const or not to const ?

```
float temp = a + b;
for (int i = 0; i < size; ++i)
    fooBar(temp)
```

```
const float temp = a + b;
for (int i = 0; i < size; ++i)
    fooBar(temp)
```

---

It doesn't matter for the compiler

(the compiler knows better than you what is const and what is not)

**const** could however make the code safer, so use it

---

```
const float input[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 };
float output[COUNT_OF(input)];
int main(int, char**) {
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

```
float input[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 };
float output[COUNT_OF(input)];
int main(int, char**) {
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

---

```
typedef const float (& immutable_array_of_16_floats)[16];
struct InputArray {
    float array[2];
    InputArray() { array[0] = 1; array[1] = 2; }
    operator immutable_array_of_16_floats () const {
        return array;
    }
};
const InputArray input;
float output[2];
int main(int, char**){
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

---

```
#include <iostream> //malloc(196);
typedef const float (& immutable_array_of_16_floats)[16];
struct InputArray {
    float array[2];
    InputArray() { array[0] = 1; array[1] = 2; }
    operator immutable_array_of_16_floats () const {
        return array;
    }
};
const InputArray input;
float output[2];
int main(int, char**){
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

---

#DEMO [CONST PROP]

---

LLVM coding style guide


```
#include <iostream>

std::cout << "Hello world";
```


>The use of #include <iostream> in library files is **hereby forbidden**

Why ?

---


```
float x, y, z;
x = atan2(e0, e1);
y = atan2(e0, e1);
z = atan2(e0, e1);
```

Is there a problem with that ?

```
for (int i = 0; i < size; ++i) {
    //..
    x += sinf(y);
    //...
}
```

```
std::string str("HPC FMI 2015);
str.length(); //how fast is that ?
```

---

O(n^2)
```
for (int i = 0; i < strlen(str); ++i)
    str[i] = toLower(str[i]);
```

O(n)
```
auto len = strlen(str);
for (int i = 0; i < len; ++i) {
    str[i] = toLower(str[i]);
}
```

---


What about **virtual** ? How is it implemented ?

Usually compiler transfer **foo.bar()** to [jump] and **foo.x** to [addr + offsetof(x)]. Where should virtual jump is known runtime only.

![virtual memory layout](../01/images/layout.png)

---

1st 64bit used for pointer to vtable (= cache miss)

can't be inlined (since what has to be executed is know only runtime)

The vptr may be re-written multiple times

There may be multiple vptrs

---

```
#define SLOW virtual
class Bar {
    SLOW void test() const;
};
```

The **virtual** keyword is the fundament of OOP.

Think **twice**     before using OOP (more on that after few weeks)

If you have array of Object*, it might make sense to sort it by dynamic type (icache works much better)

---

#[DEMO VIRTUAL]

---

Devirtualizer

```
struct A {
    virtual int foo (void) {return 42;}
};
int test() {
    struct A a, *b=&a;
    return b->foo();
}
```

Inlining + Constant propagation + Dead code removal + Inliner + Dead code removal

```
int test() () {
    return 42;
}
```

---

How can we improve the devirtualization ?

---

#STD SORT CASE STUDY

---

```
__restart:
difference_type __len = __last - __first;
switch (__len) {
    case 0: case 1:
        return;
    case 2:
        if (__comp(*--__last, *__first))
            swap(*__first, *__last);
        return;
    case 3:
        _VSTD::__sort3<_Compare>(__first, __first+1, --__last, __comp);
        return;
    case 4:
        _VSTD::__sort4<_Compare>(__first, __first+1, __first+2, --__last, __comp);
        return;
    case 5:
        _VSTD::__sort5<_Compare>(__first, __first+1, __first+2, __first+3, --__last, __comp);
        return;
    }
// const difference_type __limit = is_trivially_copy_constructible<value_type>::value && is_trivially_copy_assignable<value_type>::value ? 30 : 6;
    if (__len <= __limit) {
        _VSTD::__insertion_sort_3<_Compare>(__first, __last, __comp);
        return ;
    }
    //etcß
```

---

32bit vs 64bit CPUs

![](./images/tegra64.png)

---

Conclusion

* Single threaded performance is not getting any faster

* In the past it was hard to make complex app, but it was getting faster year after year.

* Now, sophisticated apps are easier to make (use SophisticatedAdd.js). Fast apps are *not*.

* Most of the people think that the computers are so fast, that there is no point in in-depth optimizations. As we saw, it is exactly *the opposite*.

* You need max performance per thread, use all the threads & use any extra compute units if you want to do more.

---

# Q&A

---

Note:
Since I am not sure how much time the above will take, some bonus slides in case we finish up quikcly

---

| Instruction        | ~Cycles            |
| ------------- |:-------------:|
| sqrt| 30 |
| div | 40 |
| sin | 100 |
| cos | 100 |
| sincos | 150 |
| log | 150 |
| mul | 4   |

---

#BIT HACKS

---

```
//actually slower
//why ?
void swapXOR(int& a, int& b) {
    a = a ^ b;
    b = a ^ b;
    a = a ^ b;
}
//(x ^ y) ^ y = x
//--------------------------------------------
//|input    | 0        | 1        | 2        |
//|------------------------------------------|
//|10111101 | 10010011 | 10010011 | 00101110 |
//|00101110 | 00101110 | 10111101 | 10111101 |
//|-------------------------------------------
```

---

```
//unpredictable branch?
int min0(int x, int y) {
    if (x < y)
        return x;
    return y;
}

//if x < y, -(x<y)=-1, which is all ones in two's complement represent => y^(x^y)=x
//4 cycles = L1 cache access
int min1(int x, int y) {
    return y ^ ((x^y) & - (x < y));
}
```

---

```
int mod0(int x, int y, int n) {
    return (x + y) % n;
}

// (x + y) mod n, 0 <= x < n, 0 <= y< n
int mod1(int x, int y, int n) {
    int z = x + y;
    //unpredictable branch
    return r = (z < n) ? z : z - n;
}

int mod2(int x, int y, int n) {
    int z = x + y;
    //same as minus
    return z - (n & - (z >= n));
}
```

---

```
int roundUpToPowerOf2(int64_t n) {
    // 10010
    //in case if power of 2, decrement
    --n;         // 10001
    n |= n >> 1; // 11001
    n |= n >> 2; // 11101
    n |= n >> 4; // 11111
    n |= n >> 8; //
    n |= n >> 16;//
    n |= n >> 32;//
    ++n;         //100000
    return n;
}
```

---

Quiz:

* How to find if a variable is on the stack or on the heap ?

* What is the overhead of the RAII pattern ?

---

# Common optimization approaches

---

If you have to have sentinel integers, prefer 0, 1.

```
enum {
    Invalid = -1,
    Success = 0,
    Error = 1,
};
```

vs

```
enum {
    Invalid = 100,
    Success = 200,
    Error   = 300,
};
```

Sometimes comparing with **0, 1** is faster.

Note:
https://www.youtube.com/watch?v=ea5DiCg8HOY

---

Caching

 * Function is expensive
 * Function is heavily used
 * Argument space is small
 * Results only depend on arguments
 * Function has no side effects
 * Function is deterministic

---

Lazy evaluation

* Only a few results of a large computation is needed
* Accessing the result can be done by a function call
* All that needed to calculate the results will remain unchanged 

---

Packing

* If space is more important 
* More of an old-days techinique

---

```
int data[SIZE + 1]; 
int find(int val) { 
    data[SIZE] = val;
    while (data[i] != val) 
        ++i;
    return (i == SIZE) ? -1 : i;
}
```

---

#MATH

---

![](./images/math.jpg)

---

| Before        | After            |
| ------------- |:-------------:|
| sqrt(x) > 0     | x > 0|
| sqrt(x\*x+y\*y) < sqrt(z\*z + w\*w)  | x\*x + y\*y < z\*z + w\*w     |
| log(A) + log(B) | log(A\*B)      |

---

```
return a * 4; // leaq 0(.%rdi, 4), %rax
//
return a * 43; // leaq (%rdi, %rdi, 4), %rax ; a+4*a = 5a
               // leaq (%rdi, %rax, 4), %rax ; a+4*5*a = 21a
               // leaq (%rdi, %rax, 2), %rax ; a+2*21a

```

Mult by 4/8 is part of the addresing mechanism (read, is **fast**)

---

What generally should be avoided ?

* Function calls (virtual, function ptrs)
* Upredictable branches
* Large switch statements
* Loops with exits and breaks
* Loops with unknown bounds (pragma can help)
* Memory access is assumed to have possible side effects

---

Sadly, **STL**

* First and foremost, because of the ABI

* Second
 * It is too generall in some cases (atomic counters).
 * This makes it slow. And it lacks stuff, too.
 * It is inconsistent even between minor compiler versions.
 * And, to be honest, it is not fun to use.
* Boost ?
 * No.
* Most of the companies have their in-house libraries ...


---

# OS

---

What is the OS doing ?

How can we have multiple processes when we have one core ?

How can we have multiple threads when we have one core ?

What is a **system process** and what is a **user process** ?

How does this affect the performance of your programs ?

---

How are drivers implemented ?

What happens when we call **fopen(...)** ?

How does this affect the performance of your programs ?

---

How does the scheduling of the processes happens ?

How often is it being done ?

How does this affect the performance of your programs ?

---

What is megakernel OS and what is microkernel OS ?

What kind of OSes are OS X, Windows, Linux, iOS, Android ?

How does this affect the performance of your programs ?

---

What about consoles ?

