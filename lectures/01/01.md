# HPC Intro

>“Software does not run in a magic fairy ether powered by the fevered dreams of
>CS PhDs” Branimir Karadzic

---

![Diode, P/N junction](./images/diode.png)
--
![Transistor, P/N junction](./images/transistor.png)

---

Logical AND

![Logical AND, P/N junction](./images/logical_and.gif)

---

## Закон на Мур

> Броят на транзисторите в един чип се удвоява приблизително всеки 12/18/24 месеца

Валиден ли е все още ?

---

![Moores Law](./images/moore.png)

---

Да, но ...

---

Нещо се случва през 2005

![Free Lunch Is Over](./images/free_lunch_is_over.png)

---


Защо компютрите вече не стават по-бързи ?

* Не стават ли ?

* До преди 2005 - на всеки Х месеца броят на транзисторите се удвоява по закона
  на Мур

* Това се случва посредством намаляне на размера на транзистора на чипа двойно

* Когато чипът е два пъти по-малък можем да вдигнем честотата два пъти

---

* size **L1 = L/2**

* voltage **V1 = V/2**

* frequency **F1 = 2F**

* density **D1 = 4D**

* power = **L \* V \* V \* F **
 * **P1 = 1/2 \* (1/2 \* 1/2 \* 2 \*2 \* 2)**
 * => **P1 = P**

 Обаче ...

---

Поради физични явления, вече не можем да намаляме **V**. Тоест ..


* size **L1 = L/2**

* voltage  <span style="color:red;">**V1 = V/2**</span>

* frequency **F1 = 2F**

* density **D1 = 4D**

* power = **L \* V \* V \* F **
 * **P1 = 1/2 \* 2 \*2 \* 2**
 * => <span style="color:red;">**P1 = 4P**</span>


---

**Static Power** - leakage of transistors while being inactive

**Dynamic Power** - energy needed to switch gate

---

 ![Leakage](./images/leakage.png)

---

Ват на кв. см. в микропроцесорите

 ![Power per cm^2](./images/power_per_cm2.png)

---

![Taylor, 2OO9](./images/Taylor_2OO9.png)

---

# Как работи процесора ?

---

Straight forward:

1. Fetch
2. Decode
3. Execute
4. Write-back

<img src="./images/pipeline1.svg" alt="lighterra.com" style="width: 685px;"/>

---

Има **3 начина** да получим по-голяма производителност - и това не се отнася
само до програмирането, ами изобщо

1. Работи по-усърдно / Work Harder

2. Работи по-умно / Work Smarter

3. Повикай помощ / Get Help

---

В контекста на процесорите:

1. Увеличена честота

2. Кешове / branch prediction / out of order / etc

3. Multicore / ILP / Auto SIMD


---

Pipeline

Do the CPU phases in parallel - write back result, as executing something else,
as decoding 3rd, as fetching 4th ..

<img src="./images/pipeline2.svg" alt="lighterra.com" style="width: 685px;"/>

Какви проблеми може да създаде това ?

---

Resolving dependencies

```
       for (int i = 0; i < numItems; ++i) {
            x = y[i] * z;    /// <----
            q += z + x + x;
            x = a + b;       /// <----
        }

        //////////////

        for (int i =0 ; i < numItems; ++i) {
            float x0 = y[i] * z;
            q += z + x0 + x0;
            x = a + b;
        }
```

---

Resolving dependencies in **reduction**
```
for (int i = 0; i < size; ++i)
    m = max(arr[i], m);
```
Lets unroll ..
```
for (int i = 0; i < size / 4; i+=4) {
    m = max(arr[i], m);
    m = max(arr[i+1], m); /// <----
    m = max(arr[i+2], m); /// <----
    m = max(arr[i+3], m); /// <----
}
```
Fixed:
```
m0=m1=m2=m3=-inf;
for (int i = 0; i < size/4; i+=4) {
    m0 = max(m0, arr[i+0]);
    m1 = max(m1, arr[i+1]);
    m2 = max(m2, arr[i+2]);
    m3 = max(m3, arr[i+3]);
}
m = max(m0, m1, m2, m3);
```


---

Lets assume we live in a perfect world, where CPUs are not power limited.

How fast can a CPU go ? 

---

100ghz sounds fair

100ghz = 100 000 000 000 hz = 1e11hz -> 1e-11s (= 0.01ns)

Най-дългото разстояние, което може да бъде преминато за това време е 1е-11s * 300km/s = 3mm

---

## 3mm

Максималната често е ограничена от скороста на най-бавният елемент в конвейра(пайплайна).

---

Superpipelined processors

<img src="./images/superpipelined.png" alt="lighterra.com" style="width: 685px;"/>

Какви проблеми може да създаде това ?

---

ILP - Instruction Level Parallelism

Super scalar processors

"Широчина на процесор"

<img src="./images/superscalar.png" alt="lighterra.com" style="width: 685px;"/>

Какви проблеми може да създаде това ?

---

Superpipelined Superscalar Processor

<img src="./images/superpipelinedsuperscalar.png" alt="lighterra.com" style="width: 685px;"/>

---

Resolving dependencies compile time ...

* Structural dependencies

* Data dependencies

* Controll dependencies

---

Data dependencies detection is expensive.

Black Silicon (N.B. **SILICON != SILICONE**)

---

VLIW 

<img src="./images/vliw.png" alt="lighterra.com" style="width: 685px;"/>

Old AMD GPUs

nVidia Tegra K1

Intel Itanium

Elbrus

---


1. Валът на мощността (power wall)

2. Валът на паметта (memory wall)

3. Валът на имплицитният паралелизъм (ILP wall)

<img src="./images/wall.png" alt="the wall" style="width: 685px;"/>

---


| Old        | New           |
| ------------- |:-------------:|
| Power is free, transistors are expensive      | Power Wall |
| Only dynamic power counts    | Static leakage is 40%      |
| Multiply is slow, load-and-store is fast | Memory wall      |
| ILP get better all the time, load-and-store is fast | ILP wall |
| Parallelization is not worth it | 75% per year before, 15% per year now    |
| Processor frequency goes up every X months | Number of cores goe up every X months      |

---

* Single core performance per year ~(2 to 15)% per year

* Multicore performance increase ~75% per year

---

FMAD double = 50pj

Moving 64b 1mm = 25pj

___

<img style="float: right;" src="./images/gpu.png" width="400px">

64b 10mm = 250pj

64b "off chip" = 1000pj

64b DRAM = 10000pj

12 TFLOPS ?= 300W
___

*"ALUs are like kids these days - easy to acquire, hard to feed" (B. Dally)*

---

“The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software”
(2005, Sutter)

“30x CPU performance increases in the next 50 years” (2500x for the last 30
years)

“100 GHz CPU == Чип с размер 2 милиметра”

**Производителност = Паралелизъм**

**Ефикасност = Локалност**

Еднонишково оптимизираните процесори (и езици!) са отрицание на това ("светът е
плосък")

---

## Производителност = Паралелизъм

___

## Ефикасност = Локалност

---

## Ефективно, ефикасно или производително ? Има ли разлика въобще ?

---

![cpu memory gap](./images/processor_dram_latency_gap.png)

---

RAM is slow, so lets put faster caches near to the CPU

Automatically caches memory and instructions                                    
                                                                      
![cpu cache](./images/cpu_cache.png)


---

![memory latency](./images/latency.jpg)

---

Why don't we put only caches ?

Speed of light - bigger the cache, longer the latency

From CPU to RAM (~30cm) and back takes 2ns !

SRAM vs DRAM vs GDRAM vs eDRAM vs HBM ?

---

<img style="float: right;" src="./images/memory_cell_size.png" width="400px">


SRAM
 
 * 6 transistors
 
 * May run on CPU clocks 
 
 * Fast, but hot
 
DRAM

 * 1 transistor, 1 capacitor (production of C is nice)
  
 * Much slower (<300MHZ as of today)
 
 * Slow, but dense 
 



---

![memory sram](./images/sram.png)


![memory dram](./images/dram.gif)

---

In a nutshel - two significantly different technologies

Both of them implemented as a 2D array (**reduce distance**)

Reading from DRAM goes through the **DMA and FSB** 
Row & Column ID, decode. Reading a row is faster.

"My computer has **1033/1600/1866MHZ**, so you guys are talking about some DRAMs for the past, right?"

---

## Wrong.

![memory freq](./images/memory_freq.jpg)

It is just marketing done well.

---

Okay, so what about GDRAM and eDRAM ?

<img style="float: left;" src="./images/gdram.jpg" width="380px">

<img style="float: right;" src="./images/edram.png" width="380px">

---

HBM

![memory dram](./images/hbm.png)

---

todo : TLB, branch prediction, register renaming, OoO